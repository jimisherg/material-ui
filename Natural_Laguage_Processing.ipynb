{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBBAGmbBJCXpTMsbLZdEHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimisherg/material-ui/blob/master/Natural_Laguage_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II2WEuGabX5r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "p = pos_tag(y)\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFvAKv-Gzy10",
        "outputId": "60907dcd-65f9-4305-d05b-e74e526ef7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Always', 'NNS'), ('check', 'VBP'), ('tumhara', 'JJ'), ('actual', 'JJ'), ('usage', 'NN'), ('to', 'TO'), ('avoid', 'VB'), ('unexpected', 'JJ'), ('charges', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dk3yRuX-4Vll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text PreProcessing**"
      ],
      "metadata": {
        "id": "DUec2QPy4tqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. \"\n",
        "    \"It is primarily concerned with providing computers with the ability to process data encoded in natural language. \"\n",
        "    \"Thus, it is closely related to information retrieval, knowledge representation, and computational linguistics, a subfield of linguistics. \"\n",
        "    \"Typically, data is collected in text corpora using either rule-based, statistical, or neural-based approaches in machine learning and deep learning.\"\n",
        ")\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4qQbH5s43Ey",
        "outputId": "b239faae-c927-42f3-de0e-d24652e24848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language. Thus, it is closely related to information retrieval, knowledge representation, and computational linguistics, a subfield of linguistics. Typically, data is collected in text corpora using either rule-based, statistical, or neural-based approaches in machine learning and deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "sent = sent_tokenize(text)\n",
        "print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX3xteO48eAJ",
        "outputId": "5d1b6ae3-84a4-419e-b680-d147c63ec11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence.', 'It is primarily concerned with providing computers with the ability to process data encoded in natural language.', 'Thus, it is closely related to information retrieval, knowledge representation, and computational linguistics, a subfield of linguistics.', 'Typically, data is collected in text corpora using either rule-based, statistical, or neural-based approaches in machine learning and deep learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSW7pnFr9pEA",
        "outputId": "f51ba791-0f2b-4c59-97e5-25abdea334fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'an', 'interdisciplinary', 'subfield', 'of', 'computer', 'science', 'and', 'artificial', 'intelligence', '.']\n",
            "\n",
            "['It', 'is', 'primarily', 'concerned', 'with', 'providing', 'computers', 'with', 'the', 'ability', 'to', 'process', 'data', 'encoded', 'in', 'natural', 'language', '.']\n",
            "\n",
            "['Thus', ',', 'it', 'is', 'closely', 'related', 'to', 'information', 'retrieval', ',', 'knowledge', 'representation', ',', 'and', 'computational', 'linguistics', ',', 'a', 'subfield', 'of', 'linguistics', '.']\n",
            "\n",
            "['Typically', ',', 'data', 'is', 'collected', 'in', 'text', 'corpora', 'using', 'either', 'rule-based', ',', 'statistical', ',', 'or', 'neural-based', 'approaches', 'in', 'machine', 'learning', 'and', 'deep', 'learning', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0htaRe7Q0Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T03zh9fpQ1Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evFpXZPbQ2b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = word_tokenize(text)\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FjfkgUDBzXM",
        "outputId": "874a4c08-c9cd-4121-8756-b32565bfaa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'an', 'interdisciplinary', 'subfield', 'of', 'computer', 'science', 'and', 'artificial', 'intelligence', '.', 'It', 'is', 'primarily', 'concerned', 'with', 'providing', 'computers', 'with', 'the', 'ability', 'to', 'process', 'data', 'encoded', 'in', 'natural', 'language', '.', 'Thus', ',', 'it', 'is', 'closely', 'related', 'to', 'information', 'retrieval', ',', 'knowledge', 'representation', ',', 'and', 'computational', 'linguistics', ',', 'a', 'subfield', 'of', 'linguistics', '.', 'Typically', ',', 'data', 'is', 'collected', 'in', 'text', 'corpora', 'using', 'either', 'rule-based', ',', 'statistical', ',', 'or', 'neural-based', 'approaches', 'in', 'machine', 'learning', 'and', 'deep', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Sample responses for different inputs\n",
        "responses = {\n",
        "    \"hello\": [\"Hello! How are you?\", \"Hi there! Nice to meet you!\", \"Greetings! How can I help you?\"],\n",
        "    \"how are you\": [\"I'm fine, thank you!\", \"Doing well, how about you?\", \"Great! What about you?\"],\n",
        "    \"nice to meet you\": [\"Nice to meet you too!\", \"Pleasure to meet you!\", \"Happy to meet you!\"],\n",
        "}\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(user_input):\n",
        "    # Normalize input\n",
        "    normalized_input = user_input.lower().strip()\n",
        "\n",
        "    # Check for responses\n",
        "    for key in responses:\n",
        "        if key in normalized_input:\n",
        "            return random.choice(responses[key])\n",
        "\n",
        "    return \"I'm sorry, I didn't understand that.\"\n",
        "\n",
        "# User input\n",
        "user_input = input(\"Aap kaise hain? \")  # You can input any sentence here\n",
        "response = generate_response(user_input)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ-ml1rrD1p_",
        "outputId": "93959cda-ce14-4dc3-8fdc-e3e4d11d7df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aap kaise hain? theek hu\n",
            "I'm sorry, I didn't understand that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var = \"An HTML element is a type of HTML (HyperText Markup Language) document component, one of several types of HTML nodes (there are also text nodes, comment nodes and others).[vague] The first used version of HTML was written by Tim Berners-Lee in 1993 and there have since been many versions of HTML. The current de facto standard is governed by the industry group WHATWG and is known as the HTML Living Standard.\";\n",
        "print(var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX1Tvyt5Q4Er",
        "outputId": "af143739-5843-4f3a-f407-707c00c62fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An HTML element is a type of HTML (HyperText Markup Language) document component, one of several types of HTML nodes (there are also text nodes, comment nodes and others).[vague] The first used version of HTML was written by Tim Berners-Lee in 1993 and there have since been many versions of HTML. The current de facto standard is governed by the industry group WHATWG and is known as the HTML Living Standard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WN-qOlphRJRt",
        "outputId": "b4e644ea-ef5d-4a75-ff23-67aa0a63fa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3693b9afef46>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7dE1mi9RktP",
        "outputId": "3cf4f346-e62f-4080-dda7-c864672f1121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHaNUBJCS2Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "list(punctuation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAz4u2c2SXFe",
        "outputId": "c20d9f19-fc3d-4a31-f88c-67a8762c9956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "stopwords = list(punctuation) + stop"
      ],
      "metadata": {
        "id": "a-lNH4WVS7mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in var:\n",
        "  if i not in stopwords:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vobVH22uT88w",
        "outputId": "6d53efa2-4a95-4f07-9d8e-327d4a5ffb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "n\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "e\n",
            "l\n",
            "e\n",
            "e\n",
            "n\n",
            " \n",
            " \n",
            " \n",
            "p\n",
            "e\n",
            " \n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "H\n",
            "p\n",
            "e\n",
            "r\n",
            "T\n",
            "e\n",
            "x\n",
            " \n",
            "M\n",
            "r\n",
            "k\n",
            "u\n",
            "p\n",
            " \n",
            "L\n",
            "n\n",
            "g\n",
            "u\n",
            "g\n",
            "e\n",
            " \n",
            "c\n",
            "u\n",
            "e\n",
            "n\n",
            " \n",
            "c\n",
            "p\n",
            "n\n",
            "e\n",
            "n\n",
            " \n",
            "n\n",
            "e\n",
            " \n",
            "f\n",
            " \n",
            "e\n",
            "v\n",
            "e\n",
            "r\n",
            "l\n",
            " \n",
            "p\n",
            "e\n",
            " \n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "n\n",
            "e\n",
            " \n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "r\n",
            "e\n",
            " \n",
            "l\n",
            " \n",
            "e\n",
            "x\n",
            " \n",
            "n\n",
            "e\n",
            " \n",
            "c\n",
            "e\n",
            "n\n",
            " \n",
            "n\n",
            "e\n",
            " \n",
            "n\n",
            " \n",
            "h\n",
            "e\n",
            "r\n",
            "v\n",
            "g\n",
            "u\n",
            "e\n",
            " \n",
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "f\n",
            "r\n",
            " \n",
            "u\n",
            "e\n",
            " \n",
            "v\n",
            "e\n",
            "r\n",
            "n\n",
            " \n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "w\n",
            " \n",
            "w\n",
            "r\n",
            "e\n",
            "n\n",
            " \n",
            "b\n",
            " \n",
            "T\n",
            " \n",
            "B\n",
            "e\n",
            "r\n",
            "n\n",
            "e\n",
            "r\n",
            "L\n",
            "e\n",
            "e\n",
            " \n",
            "n\n",
            " \n",
            "1\n",
            "9\n",
            "9\n",
            "3\n",
            " \n",
            "n\n",
            " \n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "h\n",
            "v\n",
            "e\n",
            " \n",
            "n\n",
            "c\n",
            "e\n",
            " \n",
            "b\n",
            "e\n",
            "e\n",
            "n\n",
            " \n",
            "n\n",
            " \n",
            "v\n",
            "e\n",
            "r\n",
            "n\n",
            " \n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "c\n",
            "u\n",
            "r\n",
            "r\n",
            "e\n",
            "n\n",
            " \n",
            "e\n",
            " \n",
            "f\n",
            "c\n",
            " \n",
            "n\n",
            "r\n",
            " \n",
            " \n",
            "g\n",
            "v\n",
            "e\n",
            "r\n",
            "n\n",
            "e\n",
            " \n",
            "b\n",
            " \n",
            "h\n",
            "e\n",
            " \n",
            "n\n",
            "u\n",
            "r\n",
            " \n",
            "g\n",
            "r\n",
            "u\n",
            "p\n",
            " \n",
            "W\n",
            "H\n",
            "A\n",
            "T\n",
            "W\n",
            "G\n",
            " \n",
            "n\n",
            " \n",
            " \n",
            "k\n",
            "n\n",
            "w\n",
            "n\n",
            " \n",
            " \n",
            "h\n",
            "e\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "L\n",
            "v\n",
            "n\n",
            "g\n",
            " \n",
            "S\n",
            "n\n",
            "r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNjeBZ-PVO2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "var_new =  word_tokenize(var)\n"
      ],
      "metadata": {
        "id": "1PmPDrGqUTgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in var:\n",
        "  if i not in var_new:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlZbN-YsVTEf",
        "outputId": "f6cb2984-0033-4191-f6fd-7ab7d891b263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "n\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "e\n",
            "l\n",
            "e\n",
            "m\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            " \n",
            "t\n",
            "y\n",
            "p\n",
            "e\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "H\n",
            "y\n",
            "p\n",
            "e\n",
            "r\n",
            "T\n",
            "e\n",
            "x\n",
            "t\n",
            " \n",
            "M\n",
            "r\n",
            "k\n",
            "u\n",
            "p\n",
            " \n",
            "L\n",
            "n\n",
            "g\n",
            "u\n",
            "g\n",
            "e\n",
            " \n",
            "d\n",
            "o\n",
            "c\n",
            "u\n",
            "m\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "c\n",
            "o\n",
            "m\n",
            "p\n",
            "o\n",
            "n\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "o\n",
            "n\n",
            "e\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "s\n",
            "e\n",
            "v\n",
            "e\n",
            "r\n",
            "l\n",
            " \n",
            "t\n",
            "y\n",
            "p\n",
            "e\n",
            "s\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "n\n",
            "o\n",
            "d\n",
            "e\n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "r\n",
            "e\n",
            " \n",
            "l\n",
            "s\n",
            "o\n",
            " \n",
            "t\n",
            "e\n",
            "x\n",
            "t\n",
            " \n",
            "n\n",
            "o\n",
            "d\n",
            "e\n",
            "s\n",
            " \n",
            "c\n",
            "o\n",
            "m\n",
            "m\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "n\n",
            "o\n",
            "d\n",
            "e\n",
            "s\n",
            " \n",
            "n\n",
            "d\n",
            " \n",
            "o\n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            "s\n",
            "v\n",
            "g\n",
            "u\n",
            "e\n",
            " \n",
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "f\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "u\n",
            "s\n",
            "e\n",
            "d\n",
            " \n",
            "v\n",
            "e\n",
            "r\n",
            "s\n",
            "i\n",
            "o\n",
            "n\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "w\n",
            "s\n",
            " \n",
            "w\n",
            "r\n",
            "i\n",
            "t\n",
            "t\n",
            "e\n",
            "n\n",
            " \n",
            "b\n",
            "y\n",
            " \n",
            "T\n",
            "i\n",
            "m\n",
            " \n",
            "B\n",
            "e\n",
            "r\n",
            "n\n",
            "e\n",
            "r\n",
            "s\n",
            "-\n",
            "L\n",
            "e\n",
            "e\n",
            " \n",
            "i\n",
            "n\n",
            " \n",
            "1\n",
            "9\n",
            "9\n",
            "3\n",
            " \n",
            "n\n",
            "d\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "h\n",
            "v\n",
            "e\n",
            " \n",
            "s\n",
            "i\n",
            "n\n",
            "c\n",
            "e\n",
            " \n",
            "b\n",
            "e\n",
            "e\n",
            "n\n",
            " \n",
            "m\n",
            "n\n",
            "y\n",
            " \n",
            "v\n",
            "e\n",
            "r\n",
            "s\n",
            "i\n",
            "o\n",
            "n\n",
            "s\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "c\n",
            "u\n",
            "r\n",
            "r\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "d\n",
            "e\n",
            " \n",
            "f\n",
            "c\n",
            "t\n",
            "o\n",
            " \n",
            "s\n",
            "t\n",
            "n\n",
            "d\n",
            "r\n",
            "d\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            "g\n",
            "o\n",
            "v\n",
            "e\n",
            "r\n",
            "n\n",
            "e\n",
            "d\n",
            " \n",
            "b\n",
            "y\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "i\n",
            "n\n",
            "d\n",
            "u\n",
            "s\n",
            "t\n",
            "r\n",
            "y\n",
            " \n",
            "g\n",
            "r\n",
            "o\n",
            "u\n",
            "p\n",
            " \n",
            "W\n",
            "H\n",
            "A\n",
            "T\n",
            "W\n",
            "G\n",
            " \n",
            "n\n",
            "d\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            "k\n",
            "n\n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "H\n",
            "T\n",
            "M\n",
            "L\n",
            " \n",
            "L\n",
            "i\n",
            "v\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "S\n",
            "t\n",
            "n\n",
            "d\n",
            "r\n",
            "d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample input string\n",
        "var = \"An HTML element is a type of HTML (HyperText Markup Language) document component, one of several types of HTML nodes (there are also text nodes, comment nodes and others).[vague] The first used version of HTML was written by Tim Berners-Lee in 1993 and there have since been many versions of HTML. The current de facto standard is governed by the industry group WHATWG and is known as the HTML Living Standard.\"\n",
        "\n",
        "# Download stopwords if you haven't already\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Get stopwords and punctuation\n",
        "stop = stopwords.words('english')\n",
        "stopwords_combined = list(punctuation) + stop\n",
        "\n",
        "# Tokenize the input string\n",
        "var_new = word_tokenize(var)\n",
        "\n",
        "# Filter and print words that are not in the stopwords_combined\n",
        "for i in var_new:\n",
        "    if i.lower() not in stopwords_combined:\n",
        "        print(type(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRsjBlZgWETd",
        "outputId": "03df3a04-339f-4b5e-e5c9-989f39c58e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer,RegexpStemmer,SnowballStemmer,PorterStemmer\n",
        "L = LancasterStemmer()\n",
        "R = RegexpStemmer('ing');\n",
        "P = PorterStemmer()\n",
        "S = SnowballStemmer('english')\n",
        "L.stem('changed')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dur7mRg0XQXS",
        "outputId": "c9e39fcd-8285-4143-804d-f55f54aeabab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R.stem('changed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XPmZqb7mZTRW",
        "outputId": "88e4fdcc-ebd2-4beb-a48c-7703eb4089a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'changed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = PorterStemmer()\n",
        "P.stem('changed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8kOc9NLvZbvb",
        "outputId": "c962ef93-dc9f-4156-d82e-ed31950cb1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S = SnowballStemmer('english')\n",
        "S.stem('changing')"
      ],
      "metadata": {
        "id": "0YHuq3bdZioi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`**Leminitization**`\n",
        "\n",
        "Lemmatization is the process of reducing a word to its base or root form, which is known as a \"lemma.\" Unlike stemming, which simply chops off prefixes or suffixes, lemmatization takes into account the meaning of the word and its context, resulting in a valid word.\n",
        "\n",
        "Example:\n",
        "The lemma of \"running\" is \"run.\"\n",
        "The lemma of \"better\" is \"good.\"\n",
        "Using NLTK for Lemmatization\n",
        "Here's how you can perform lemmatization using NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download WordNet if you haven't already\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"ran\", \"better\", \"mice\", \"geese\", \"children\"]\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Print the results\n",
        "print(lemmatized_words)\n",
        "Explanation:\n",
        "WordNetLemmatizer: This class provides the lemmatization functionality.\n",
        "Lemmatization Process: You can apply the lemmatize method on each word to convert it to its lemma.\n",
        "Notes:\n",
        "Lemmatization may require specifying the part of speech (POS) for more accurate results. By default, it assumes nouns. For example:\n",
        "python\n",
        "Copy code\n",
        "lemmatizer.lemmatize(\"better\", pos='a')  # 'a' for adjective\n",
        "Make sure to explore how the context of the word can affect its lemma for more advanced applications!\n",
        "\n"
      ],
      "metadata": {
        "id": "ZvJKnwDJbZUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download WordNet if you haven't already\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize some valid words\n",
        "print(wl.lemmatize(\"mice\"))  # Output: 'mouse'\n",
        "print(wl.lemmatize(\"better\", pos='a'))  # Output: 'good'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gNjbNvLbxql",
        "outputId": "ac57a214-6fda-464c-f51b-fb2698323ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mouse\n",
            "good\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am Anmol Mahajan  I am a lrarning lover to thionks as rel;ated to computer  I Like kulcha chola  hello i am common man\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "word = word_tokenize(text)\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja0j4D03gZmx",
        "outputId": "d61ec6eb-1108-4327-d7c8-91cca22dc9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'Anmol', 'Mahajan', 'I', 'am', 'a', 'lrarning', 'lover', 'to', 'thionks', 'as', 'rel', ';', 'ated', 'to', 'computer', 'I', 'Like', 'kulcha', 'chola', 'hello', 'i', 'am', 'common', 'man']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import *\n",
        "B = BigramCollocationFinder.from_words(text)\n",
        "B.ngram_fd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QacTvd9XhJyU",
        "outputId": "61de21d9-4a6c-4bc6-f76d-9da973cd4842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({(' ', 'a'): 5, ('I', ' '): 3, ('a', 'm'): 3, ('m', ' '): 3, (' ', ' '): 3, ('a', ' '): 3, (' ', 't'): 3, ('o', ' '): 3, (' ', 'c'): 3, ('m', 'o'): 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "# Your text\n",
        "text = \"I am Anmol Mahajan. I am a learning lover and think as related to computer. I like kulcha chola. Hello, I am a common man.\"\n",
        "\n",
        "# Tokenizing the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Finding bigrams\n",
        "bigram_finder = BigramCollocationFinder.from_words(words)\n",
        "\n",
        "# Getting the frequency distribution of bigrams\n",
        "bigrams = bigram_finder.ngram_fd.items()\n",
        "\n",
        "# Printing the results\n",
        "for bigram, freq in bigrams:\n",
        "    print(f\"{bigram}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQmGu-WxjNYo",
        "outputId": "9f753f56-4917-4834-e42b-3057b8a24f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'am'): 3\n",
            "('am', 'Anmol'): 1\n",
            "('Anmol', 'Mahajan'): 1\n",
            "('Mahajan', '.'): 1\n",
            "('.', 'I'): 2\n",
            "('am', 'a'): 2\n",
            "('a', 'learning'): 1\n",
            "('learning', 'lover'): 1\n",
            "('lover', 'and'): 1\n",
            "('and', 'think'): 1\n",
            "('think', 'as'): 1\n",
            "('as', 'related'): 1\n",
            "('related', 'to'): 1\n",
            "('to', 'computer'): 1\n",
            "('computer', '.'): 1\n",
            "('I', 'like'): 1\n",
            "('like', 'kulcha'): 1\n",
            "('kulcha', 'chola'): 1\n",
            "('chola', '.'): 1\n",
            "('.', 'Hello'): 1\n",
            "('Hello', ','): 1\n",
            "(',', 'I'): 1\n",
            "('a', 'common'): 1\n",
            "('common', 'man'): 1\n",
            "('man', '.'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B.ngram_fd.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RxopzrSir-T",
        "outputId": "366ed46c-8100-4eaf-ef0f-dd15c87a245d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([('I', ' '), (' ', 'a'), ('a', 'm'), ('m', ' '), (' ', 'A'), ('A', 'n'), ('n', 'm'), ('m', 'o'), ('o', 'l'), ('l', ' '), (' ', 'M'), ('M', 'a'), ('a', 'h'), ('h', 'a'), ('a', 'j'), ('j', 'a'), ('a', 'n'), ('n', ' '), (' ', ' '), (' ', 'I'), ('a', ' '), (' ', 'l'), ('l', 'r'), ('r', 'a'), ('a', 'r'), ('r', 'n'), ('n', 'i'), ('i', 'n'), ('n', 'g'), ('g', ' '), ('l', 'o'), ('o', 'v'), ('v', 'e'), ('e', 'r'), ('r', ' '), (' ', 't'), ('t', 'o'), ('o', ' '), ('t', 'h'), ('h', 'i'), ('i', 'o'), ('o', 'n'), ('n', 'k'), ('k', 's'), ('s', ' '), ('a', 's'), (' ', 'r'), ('r', 'e'), ('e', 'l'), ('l', ';'), (';', 'a'), ('a', 't'), ('t', 'e'), ('e', 'd'), ('d', ' '), (' ', 'c'), ('c', 'o'), ('o', 'm'), ('m', 'p'), ('p', 'u'), ('u', 't'), (' ', 'L'), ('L', 'i'), ('i', 'k'), ('k', 'e'), ('e', ' '), (' ', 'k'), ('k', 'u'), ('u', 'l'), ('l', 'c'), ('c', 'h'), ('h', 'o'), ('l', 'a'), (' ', 'h'), ('h', 'e'), ('l', 'l'), (' ', 'i'), ('i', ' '), ('m', 'm'), (' ', 'm'), ('m', 'a')])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"my name is Anmol Mahajan\",\"my nbame is dhriuve\"]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"name\":l})\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "TKNZwyfsiO_3",
        "outputId": "7411fc46-bcf4-4fc8-a544-bc81d202ef77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       name\n",
              "0  my name is Anmol Mahajan\n",
              "1       my nbame is dhriuve"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb45d53c-0528-4891-9aa9-938fb2f4daf1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my name is Anmol Mahajan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my nbame is dhriuve</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb45d53c-0528-4891-9aa9-938fb2f4daf1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb45d53c-0528-4891-9aa9-938fb2f4daf1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb45d53c-0528-4891-9aa9-938fb2f4daf1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1fffc23c-6c6d-459d-b7d5-9768ec87ab45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fffc23c-6c6d-459d-b7d5-9768ec87ab45')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1fffc23c-6c6d-459d-b7d5-9768ec87ab45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b43fc0a3-a0e3-4896-bec4-c2aa51175d47\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b43fc0a3-a0e3-4896-bec4-c2aa51175d47 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"my nbame is dhriuve\",\n          \"my name is Anmol Mahajan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVOE-seRuwHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "new_data = cv.fit_transform(df[\"name\"])\n",
        "new_data.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "q-loMykjttK2",
        "outputId": "3a74e41a-0c5a-445d-c61d-ddfda59c826f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'CountVectorizer' from 'sklearn.feature_extraction' (/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b0642ff011d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CountVectorizer' from 'sklearn.feature_extraction' (/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example list of names (replace with your own data)\n",
        "l = [\"Alice\", \"Bob\", \"Alice Smith\", \"Charlie\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\"name\": l})\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "new_data = cv.fit_transform(df[\"name\"])\n",
        "\n",
        "# Convert to array\n",
        "new_array = new_data.toarray()\n",
        "\n",
        "# Print the array\n",
        "print(new_array)\n",
        "cv.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocVbusW9uxNE",
        "outputId": "e4ea227b-b7d0-429f-b179-00b042322b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [1 0 0 1]\n",
            " [0 0 1 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alice': 0, 'bob': 1, 'smith': 3, 'charlie': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6_OV7OO0kF1",
        "outputId": "09c35075-39b1-4c9f-afcc-c3da4361f4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "Successfully installed scikit-learn-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = \"A computer mouse is a handheld input device that allows users to interact with a computer. It moves a pointer (cursor) on the screen, enabling users to select, move, and interact with text, icons, files, and other graphical user interface (GUI) elements.\";\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "94-QbuFF0ogX",
        "outputId": "79c8a8b5-ff12-4862-ec0f-ae4a979e579a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A computer mouse is a handheld input device that allows users to interact with a computer. It moves a pointer (cursor) on the screen, enabling users to select, move, and interact with text, icons, files, and other graphical user interface (GUI) elements.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"You can reinstall or upgrade scikit-learn using pip. Run the following command in your Google Colab or local environment:\";"
      ],
      "metadata": {
        "id": "ynRX3Y1xzbev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "var = \"You can reinstall or upgrade scikit-learn using pip. Run the following command in your Google Colab or local environment:\";"
      ],
      "metadata": {
        "id": "mfwh3_7qwRRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import  lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "store  = lesk(word_tokenize(y),\"mouse\");\n",
        "store.definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QfXAGmbywzBp",
        "outputId": "476bfdf0-fd7d-44cd-e7da-a49bcb228641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    }
  ]
}